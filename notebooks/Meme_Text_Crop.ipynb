{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71eeaca8",
   "metadata": {},
   "source": [
    "## Crop Analysis for English, Arabic, and Paired English+Arabic Memes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8512606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import shlex\n",
    "import subprocess\n",
    "import sys\n",
    "import io\n",
    "import pandas as pd\n",
    "from collections import namedtuple\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.collections import PatchCollection\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a357eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "\n",
    "BIN_MAPS = {\"Darwin\": \"mac\", \"Linux\": \"linux\"}\n",
    "\n",
    "HOME_DIR = Path(\"../\").expanduser()\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    ! pip install pandas scikit-learn scikit-image statsmodels requests dash\n",
    "    ! [[ -d image-crop-analysis ]] || git clone https://github.com/twitter-research/image-crop-analysis.git\n",
    "    HOME_DIR = Path(\"./image-crop-analysis\").expanduser()\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "sys.path.append(str(HOME_DIR / \"src\"))\n",
    "bin_dir = HOME_DIR / Path(\"./bin\")\n",
    "bin_path = bin_dir / BIN_MAPS[platform.system()] / \"candidate_crops\"\n",
    "model_path = bin_dir / \"fastgaze.vxm\"\n",
    "data_dir = HOME_DIR / Path(\"./data/\")\n",
    "data_dir_plot_en = HOME_DIR / Path(\"./data_plot/En\")\n",
    "data_dir_plot_ar = HOME_DIR / Path(\"./data_plot/Ar\")\n",
    "data_dir_plot_bi = HOME_DIR / Path(\"./data_plot/En_Ar\")\n",
    "data_dir_plot_enar = HOME_DIR / Path(\"./data_plot/En_Ar_Paired\")\n",
    "data_dir_plot_aren = HOME_DIR / Path(\"./data_plot/Ar_En_Paired\")\n",
    "\n",
    "data_dir.exists()\n",
    "data_dir_plot_en.exists()\n",
    "data_dir_plot_ar.exists()\n",
    "data_dir_plot_bi.exists()\n",
    "data_dir_plot_enar.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76c76d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from image_manipulation import join_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e4017a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crop_api import ImageSaliencyModel, is_symmetric, parse_output, reservoir_sampling\n",
    "model = ImageSaliencyModel(crop_binary_path=bin_path, crop_model_path=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06dde06",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matplotlib.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7991df88",
   "metadata": {},
   "source": [
    "### Function for the Salient Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e999ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the salient point's info by defining a function\n",
    "def get_salient_info(img_path):\n",
    "    if isinstance(img_path, str):\n",
    "        img_path = Path(img_path)\n",
    "    try:\n",
    "        cmd = f\"{str(bin_path)} {str(model_path)} '{img_path.absolute()}' show_all_points\"\n",
    "        output = subprocess.check_output(cmd, shell=True)  # Success!\n",
    "        return parse_output(output)\n",
    "    except:\n",
    "        print(\"Running the model to get salient point fails. Returning None.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67618add",
   "metadata": {},
   "source": [
    "### Experiment 1.1: Analyzing English Memes Seperatly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766e5540",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,41):\n",
    "    img_path = data_dir / Path(\"./\"+str(i)+\"en.jpeg\")\n",
    "    name_en=str(i)+\"en.jpeg\"\n",
    "    model.plot_img_crops(img_path, topK=1)\n",
    "    #saving plots in the ./data_plot/En folder\"\n",
    "    plt.savefig(data_dir_plot_en/name_en, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4f5afe",
   "metadata": {},
   "source": [
    "### Experiment 1.2: Analyzing Arabic Memes Seperatly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37196b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,41):\n",
    "    img_path = data_dir / Path(\"./\"+str(i)+\"ar.jpeg\")\n",
    "    name_ar=str(i)+\"ar.jpeg\"\n",
    "    model.plot_img_crops(img_path, topK=1)\n",
    "    #saving plots in the ./data_plot/Ar folder\"\n",
    "    plt.savefig(data_dir_plot_ar/name_ar, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd84d439",
   "metadata": {},
   "source": [
    "### Experiment 1.3: Analyzing English and Arabic in one image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafe4b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,41):\n",
    "    img_path = data_dir / Path(\"./\"+str(i)+\"bi.jpeg\")\n",
    "    name_bi=str(i)+\"bi.jpeg\"\n",
    "    model.plot_img_crops(img_path, topK=1)\n",
    "    #saving plots in the ./data_plot/Ar folder\"\n",
    "    plt.savefig(data_dir_plot_bi/name_bi, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b5133d",
   "metadata": {},
   "source": [
    "### Experiment 2.1 : Analyzing paired Images, English and Arabic together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb492ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''This piece of code:\n",
    "    1) set a counter for everytime that the algorithm picks English region or Arabic region\n",
    "    2) join English and Arabic images horizontally: English on the left and Arabic on the right and give a \"*enar.jpeg\" to it\n",
    "    3) Run the saliency algorithm to see if the most salient point is in the English region or the Arabic region. if the most salient point's \"x\" value is between 0 to (imgage's width)/2, then the most salient point is in the English region otherwise it's in the Arabic region\n",
    "    4) Increase the counter for selecting English region or Arabic region and creating a csv file'''\n",
    "\n",
    "#Counter's count the number of times English or Arabic meme is selected in a joined image\n",
    "\n",
    "counter_en=0\n",
    "counter_ar=0\n",
    "region=str(\"\")\n",
    "rows=[]\n",
    "for i in range(1,41):\n",
    "    \n",
    "    # attaching English and Arabic images horizontally: English on the left and Arabic on the right\n",
    "    images = [\n",
    "    Image.open(data_dir / Path(\"./\"+str(i)+\"en.jpeg\")),\n",
    "    Image.open(data_dir / Path(\"./\"+str(i)+\"ar.jpeg\")),\n",
    "    ]\n",
    "    img = join_images(images, col_wrap=2, img_size=(500, 500), padding=1)\n",
    "    name_enar=str(i)+\"enar.jpeg\"\n",
    "    img.save(data_dir/name_enar, \"JPEG\")\n",
    "    \n",
    "    ## finding if the salient point is in the English region or the Arabic region\n",
    "\n",
    "    img_path = data_dir / Path(\"./\"+str(i)+\"enar.jpeg\")\n",
    "    model.plot_img_crops(img_path, topK=1)\n",
    "    \n",
    "    #saving plots in the ./data_plot folder\"\n",
    "    plt.savefig(data_dir_plot_enar/name_enar, bbox_inches=\"tight\")\n",
    "    plot_path=data_dir_plot_enar/name_enar\n",
    "    salient_info = get_salient_info(img_path)\n",
    "    all_salient_points = salient_info[\"salient_point\"]\n",
    "    print(\"salient info for \"+name_enar+\" is \" , all_salient_points[0])\n",
    "    if all_salient_points[0][0]<=img.width/2:\n",
    "        print(\"For image \"+name_enar,\"English Meme is Selected, because the saleint point's X value is less than half of the image's width= \"+ str(img.width) + \" and it's in the English side\")\n",
    "        region=str(\"English\")\n",
    "        counter_en+=1\n",
    "        rows.append ([name_enar, str(all_salient_points[0]), region, plot_path ])\n",
    "    else:\n",
    "        print(\"For image \"+name_enar,\"Arabic Meme is Selected, because the saleint point's X value is bigger than half of the image's width= \"+ str(img.width) + \" and it's in the Arabic side\")\n",
    "        region=str(\"Arabic\")\n",
    "        counter_ar+=1\n",
    "        rows.append ([name_enar, str(all_salient_points[0]), str(region), plot_path ])\n",
    "        \n",
    "# Making a CSV file to look at the images and the selection region for the most salient point\n",
    "print(counter_en)\n",
    "print(counter_ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a127f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the result as a csv file and also a bar chart\n",
    "df = pd.DataFrame(rows, columns=[\"Name\", \"Salient Point\", \"Selected Region\", \"Plot Directory\"])\n",
    "df.to_csv(data_dir_plot_enar/'enar_region.csv', index=False)\n",
    "selected_region=[\"English\",\"Arabic\"]\n",
    "number_selected=[counter_en, counter_ar]\n",
    "plt.bar(selected_region, number_selected)\n",
    "plt.title(\"Number of times that English or Arabic regions are selected\")\n",
    "plt.savefig(data_dir_plot_enar/\"enar_bar_plot.jpeg\")\n",
    "plt.show()\n",
    "total= (counter_en*100)/(counter_en+counter_ar)\n",
    "print(str(total)+\"% of the most salient points of total paired memes are in the English region\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03daa039",
   "metadata": {},
   "source": [
    "### Experiment 2.2: Analyzing paired Images, English and Arabic together (Arabic: Left-side, English: Right-side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f839964e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This piece of code:\n",
    "    1) set a counter for everytime that the algorithm picks English region or Arabic region\n",
    "    2) join English and Arabic images horizontally: Arabic on the left and English on the right and give a \"*aren.jpeg\" to it\n",
    "    3) Run the saliency algorithm to see if the most salient point is in the English region or the Arabic region. if the most salient point's \"x\" value is between 0 to (imgage's width)/2, then the most salient point is in the Arabic region otherwise it's in the Arabic region\n",
    "    4) Increase the counter for selecting English region or Arabic region and creating a csv file'''\n",
    "\n",
    "counter_en=0\n",
    "counter_ar=0\n",
    "region=str(\"\")\n",
    "rows=[]\n",
    "for i in range(1,41):\n",
    "    \n",
    "    # attaching English and Arabic images horizontally: English on the left and Arabic on the right\n",
    "    images = [\n",
    "        Image.open(data_dir / Path(\"./\"+str(i)+\"ar.jpeg\")),\n",
    "        Image.open(data_dir / Path(\"./\"+str(i)+\"en.jpeg\")),\n",
    "    ]\n",
    "    img = join_images(images, col_wrap=2, img_size=(500, 500), padding=1)\n",
    "    name_aren=str(i)+\"aren.jpeg\"\n",
    "    img.save(data_dir/name_aren, \"JPEG\")\n",
    "    \n",
    "    ## finding if the salient point is in the English region or the Arabic region\n",
    "\n",
    "    img_path = data_dir / Path(\"./\"+str(i)+\"aren.jpeg\")\n",
    "    model.plot_img_crops(img_path, topK=1)\n",
    "    \n",
    "    #saving plots in the ./data_plot folder\"\n",
    "    plt.savefig(data_dir_plot_aren/name_aren, bbox_inches=\"tight\")\n",
    "    plot_path=data_dir_plot_aren/name_aren\n",
    "    salient_info = get_salient_info(img_path)\n",
    "    all_salient_points = salient_info[\"salient_point\"]\n",
    "    print(\"salient info for \"+name_aren+\" is \" , all_salient_points[0])\n",
    "    if all_salient_points[0][0]<=img.width/2:\n",
    "        print(\"For image \"+name_aren,\"Arabic Meme is Selected, because the saleint point's X value is less than half of the image's width= \"+ str(img.width) + \" and it's in the Arabic side\")\n",
    "        region=str(\"Arabic\")\n",
    "        counter_ar+=1\n",
    "        rows.append ([name_aren, str(all_salient_points[0]), region, plot_path ])\n",
    "    else:\n",
    "        print(\"For image \"+name_aren,\"English Meme is Selected, because the saleint point's X value is bigger than half of the image's width= \"+ str(img.width) + \" and it's in the English side\")\n",
    "        region=str(\"English\")\n",
    "        counter_en+=1\n",
    "        rows.append ([name_aren, str(all_salient_points[0]), str(region), plot_path ])\n",
    "        \n",
    "# Making a CSV file to look at the images and the selection region for the most salient point\n",
    "print(counter_en)\n",
    "print(counter_ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea3c0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the result as a csv file and also a bar chart\n",
    "df = pd.DataFrame(rows, columns=[\"Name\", \"Salient Point\", \"Selected Region\", \"Plot Directory\"])\n",
    "df.to_csv(data_dir_plot_aren/'aren_region.csv', index=False)\n",
    "selected_region=[\"English\",\"Arabic\"]\n",
    "number_selected=[counter_en, counter_ar]\n",
    "plt.bar(selected_region, number_selected)\n",
    "plt.title(\"Number of times that English or Arabic regions are selected\")\n",
    "plt.savefig(data_dir_plot_aren/\"aren_bar_plot.jpeg\")\n",
    "plt.show()\n",
    "total= (counter_en*100)/(counter_en+counter_ar)\n",
    "print(str(total)+\"% of the most salient points of total paired memes are in the English region\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
